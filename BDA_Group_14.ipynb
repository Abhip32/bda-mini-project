{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QghG3vabFl6a"
      },
      "source": [
        "MATRIX VECTOR MULTIPLICATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGqYp_DyFlVz",
        "outputId": "5d10c54e-2829-4230-daaa-03d7d0be57f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abhishek Patil/BDA/CA-2\n",
            "Row 0: 85\n",
            "Row 1: 90\n",
            "Row 2: 43\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "# Initialize SparkContext\n",
        "conf = SparkConf().setAppName(\"MatrixVectorMultiplication\")\n",
        "#sc = SparkContext(conf=conf)\n",
        "# Input matrix and vector\n",
        "matrix = [\n",
        "(0, [8, 4, 3]),\n",
        "(1, [9, 5, 2]),\n",
        "(2, [7, 1, 1])\n",
        "]\n",
        "vector = [4, 8, 7]\n",
        "# Broadcast the vector to all nodes in the cluster\n",
        "broadcast_vector = sc.broadcast(vector)\n",
        "# Perform matrix-vector multiplication using MapReduce\n",
        "result = sc.parallelize(matrix) \\\n",
        ".map(lambda row: (row[0], sum([row[1][i] * broadcast_vector.value[i] for i in\n",
        "range(len(row[1]))]))) \\\n",
        ".collect()\n",
        "# Print the result\n",
        "print(\"Abhishek Patil/BDA/CA-2\")\n",
        "for row_id, value in sorted(result):\n",
        "  print(f\"Row {row_id}: {value}\")\n",
        "# Stop SparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregations - Mean, Sum, Std Deviation"
      ],
      "metadata": {
        "id": "ZxxmBBu2P23X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from math import sqrt\n",
        "\n",
        "# Dummy input data\n",
        "input_data = [\n",
        "    'key1\\t25',\n",
        "    'key2\\t50',\n",
        "    'key1\\t75',\n",
        "    'key2\\t100',\n",
        "    'key1\\t125',\n",
        "    'key2\\t150',\n",
        "]\n",
        "\n",
        "def map_func(line):\n",
        "    key, value = line.split('\\t')\n",
        "    return key, float(value)\n",
        "\n",
        "def reduce_func(data):\n",
        "    values = [x for x in data]\n",
        "    mean_val = sum(values) / len(values)\n",
        "    sum_val = sum(values)\n",
        "    std_dev_val = sqrt(sum((x - mean_val)**2 for x in values) / (len(values) - 1)) if len(values) > 1 else 0\n",
        "    return {\n",
        "        'mean': mean_val,\n",
        "        'sum': sum_val,\n",
        "        'std_dev': std_dev_val\n",
        "    }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #sc = SparkContext('local', 'AggregationSpark')\n",
        "    lines = sc.parallelize(input_data)\n",
        "    mapped = lines.map(map_func)\n",
        "    grouped = mapped.groupByKey()\n",
        "    result = grouped.mapValues(list).mapValues(reduce_func)\n",
        "    output = result.collect()\n",
        "    print(\"Abhishek Patil/BDA/CA-2\")\n",
        "    for key, value in output:\n",
        "        print(f'{key}\\t{value}')\n",
        "    sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FdRdl9dHlFg",
        "outputId": "f4358cc7-46f7-47c9-bef6-c0f9687e50b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abhishek Patil/BDA/CA-2\n",
            "key1\t{'mean': 75.0, 'sum': 225.0, 'std_dev': 50.0}\n",
            "key2\t{'mean': 100.0, 'sum': 300.0, 'std_dev': 50.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort the data"
      ],
      "metadata": {
        "id": "xxL00T2-P7ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"SortData\") \\\n",
        ".getOrCreate()\n",
        "# Define dummy input data\n",
        "dummy_data = [\n",
        "\"3\\tCow\",\n",
        "\"1\\tDog\",\n",
        "\"2\\tCat\",\n",
        "\"4\\tBuffalo\"\n",
        "]\n",
        "# Create RDD from dummy data\n",
        "data_rdd = spark.sparkContext.parallelize(dummy_data)\n",
        "# Sort the data\n",
        "sorted_data = data_rdd.sortBy(lambda x: x.split('\\t')[0])\n",
        "# Collect and print the sorted data\n",
        "sorted_results = sorted_data.collect()\n",
        "print(\"Abhishek Patil/BDA/CA-2\")\n",
        "for result in sorted_results:\n",
        "  print(result)\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74AKyJ8qNymw",
        "outputId": "8c8da3ad-9c8c-4543-f5a6-bf6f8e258524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abhishek Patil/BDA/CA-2\n",
            "1\tDog\n",
            "2\tCat\n",
            "3\tCow\n",
            "4\tBuffalo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search a data element"
      ],
      "metadata": {
        "id": "pYqsg5CEQABv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "# Create a Spark context\n",
        "conf = SparkConf().setAppName(\"SearchElement\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "# Define the data to be searched\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "# Parallelize the data into RDD (Resilient Distributed Dataset)\n",
        "rdd = sc.parallelize(data)\n",
        "# Define the search function\n",
        "def search_element(element):\n",
        "  return element == 11 # Change the search element as needed\n",
        "# Map function to search for the element in the dataset\n",
        "result = rdd.map(search_element)\n",
        "# Collect the results\n",
        "search_result = result.collect()\n",
        "# Print the search result\n",
        "print(\"Abhishek Patil/BDA/CA-2\")\n",
        "if True in search_result:\n",
        "  print(\"Element found in the dataset\")\n",
        "else:\n",
        "  print(\"Element not found in the dataset\")\n",
        "# Stop the Spark context\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiIOm7r0ORk8",
        "outputId": "af94e487-bdaa-47c5-fcf5-30aa7b2aa827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abhishek Patil/BDA/CA-2\n",
            "Element not found in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Joins - Map Side and Reduce Side"
      ],
      "metadata": {
        "id": "ys3sSt8RQDZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Spark for Joins - Map Side and Reduce Side\n",
        "from pyspark import SparkContext\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Joins\")\n",
        "# Create RDDs for left and right datasets\n",
        "left_data = sc.parallelize([(1, \"A\"), (2, \"B\"), (3, \"C\")])\n",
        "right_data = sc.parallelize([(1, \"X\"), (2, \"Y\"), (4, \"Z\")])\n",
        "# Perform map-side join\n",
        "map_join = left_data.join(right_data)\n",
        "# Perform reduce-side join\n",
        "reduce_join = left_data.union(right_data).reduceByKey(lambda x, y: (x, y))\n",
        "# Print the results\n",
        "print(\"Map Side Join:\", map_join.collect())\n",
        "print(\"Reduce Side Join:\", reduce_join.collect())\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfLtgZy3Pecy",
        "outputId": "d4dc027c-c482-46b3-9483-f1dbab93a243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map Side Join: [(2, ('B', 'Y')), (1, ('A', 'X'))]\n",
            "Reduce Side Join: [(2, ('B', 'Y')), (4, 'Z'), (1, ('A', 'X')), (3, 'C')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}